{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e64985e-b564-4cda-8d31-386ab13367a2",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "1. Мы будем работать с (частичными) данными lenta.ru отсюда: https://www.kaggle.com/yutkin/corpus-of-russian-news-articles-from-lenta/\n",
    "2. Проведите препроцессинг текста. Разбейте данные на train и test для задачи классификации (в качестве метки класса будем использовать поле topic). В качестве данных для классификации в пунктах 3 и 5 возьмите\n",
    "    - только заголовки (title)\n",
    "    - только тексты новости (text)\n",
    "    - и то, и другое\n",
    "3. Обучите fastText для классификации текстов по темам. Сравните качество для разных данных из п. 2.\n",
    "4. Обучите свою модель w2v (или возьмите любую подходящую предобученную модель). Реализуйте функцию для вычисления вектора текста / заголовка / текста+заголовка как среднего вектора входящих в него слов. \n",
    "     - (Бонус) Модифицируйте функцию вычисления среднего вектора: взвешивайте вектора слов соответствующими весами tf-idf.\n",
    "5. Обучите на полученных средних векторах алгоритм классификации, сравните полученное качество с классификатором fastText. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95387630-8069-494d-b3ac-6c36db7222ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "!pip install pymystem3 -q\n",
    "!pip install fasttext -q\n",
    "!pip install gensim -q\n",
    "!pip install nltk -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import fasttext\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acf7059-b9dd-402f-bf8e-db4767ab350a",
   "metadata": {},
   "source": [
    "### Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d0e64d2-1be2-4f91-a92f-91e10b60a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -O data/lenta-ru-news-part.csv https://www.dropbox.com/s/ja23c9l1ppo9ix7/lenta-ru-news-part.csv?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "503e09c1-7fd7-4316-b8db-da6247ce116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d yutkin/corpus-of-russian-news-articles-from-lenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7135ca6b-cb6b-49ee-be9b-77505a6bdc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "167192f0-f86d-41e0-b6c1-5b08df10d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip data/archive.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a9fbd5-d355-432e-ae9a-2915f425e56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
       "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
       "      <td>Библиотека</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
       "      <td>Министерство народного просвещения, в виду про...</td>\n",
       "      <td>Библиотека</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1914. Das ist Nesteroff!</td>\n",
       "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
       "      <td>Библиотека</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1914. Бульдог-гонец под Льежем</td>\n",
       "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
       "      <td>Библиотека</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
       "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
       "      <td>Библиотека</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  1914. Русские войска вступили в пределы Венгрии     \n",
       "1  1914. Празднование столетия М.Ю. Лермонтова от...   \n",
       "2                           1914. Das ist Nesteroff!   \n",
       "3                    1914. Бульдог-гонец под Льежем    \n",
       "4           1914. Под Люблином пойман швабский зверь   \n",
       "\n",
       "                                                text       topic  \n",
       "0  Бои у Сопоцкина и Друскеник закончились отступ...  Библиотека  \n",
       "1  Министерство народного просвещения, в виду про...  Библиотека  \n",
       "2  Штабс-капитан П. Н. Нестеров на днях, увидев в...  Библиотека  \n",
       "3  Фотограф-корреспондент Daily Mirror рассказыва...  Библиотека  \n",
       "4  Лица, приехавшие в Варшаву из Люблина, передаю...  Библиотека  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenta = pd.read_csv('data/lenta-ru-news.csv', usecols=['title', 'text', 'topic'], low_memory=False)\n",
    "lenta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7f703-080c-4708-aef0-3cbcb939e20c",
   "metadata": {},
   "source": [
    "#### Оставим только классы которые подразумевались в задании"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d006f085-7275-44e6-b3bc-578751ce3ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_topics = [\n",
    "    'Экономика',\n",
    "    'Спорт',\n",
    "    'Культура',\n",
    "    'Наука и техника',\n",
    "    'Бизнес'\n",
    "] \n",
    "\n",
    "lenta = lenta[lenta['topic'].isin(target_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892cc50d-30a3-4e5c-84e0-fa1ede9601f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "Экономика          79528\n",
       "Спорт              64413\n",
       "Культура           53797\n",
       "Наука и техника    53136\n",
       "Бизнес              7399\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenta.topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba00946-71a3-46cd-9d3b-782f84576f8e",
   "metadata": {},
   "source": [
    "### Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f19a160-cf3d-400e-80ea-c05e939d0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_analyzer = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bbdb3b7-f30d-4044-bd7f-88d9b2fc78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stopwords = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3612d289-7723-4074-95b1-d30501260add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    tokens = mystem_analyzer.lemmatize(text)\n",
    "    word_tokens = [token for token in tokens if token.isalpha()]\n",
    "    return word_tokens\n",
    "\n",
    "def clean_stopwords(tokens):\n",
    "    non_stopword_tokens = [token for token in tokens if token not in russian_stopwords]\n",
    "    return non_stopword_tokens\n",
    "\n",
    "def preprocess_text(text):\n",
    "    lemma_tokens = lemmatize(text.lower())\n",
    "    clean_tokens = clean_stopwords(lemma_tokens)\n",
    "    return ' '.join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5be3d205-562e-40fa-8c9b-92dd2fb9ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7760630e-5ddb-4457-97ab-3fcfde92d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta = lenta[lenta.title.apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "lenta = lenta[lenta.text.apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "lenta = lenta[lenta.title.notna()]\n",
    "\n",
    "lenta = lenta[lenta.text.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4004d824-196f-426b-9a21-246336489591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenta_lemmatized is already exists\n"
     ]
    }
   ],
   "source": [
    "lenta_lemmatized_path = os.path.join(data_dir, 'lenta_lemmatized.csv')\n",
    "\n",
    "if not os.path.exists(lenta_lemmatized_path):\n",
    "    lenta['title_lemma'] = lenta['title'].progress_apply(preprocess_text)\n",
    "\n",
    "    lenta['text_lemma'] = lenta['text'].progress_apply(preprocess_text)\n",
    "\n",
    "    lenta.to_csv(os.path.join(data_dir, 'lenta_lemmatized.csv'), index=False)\n",
    "else:\n",
    "    print(\"lenta_lemmatized is already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c741b4b4-6a8c-4829-a037-270d0c24902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta_lemmatized = pd.read_csv(os.path.join(data_dir, 'lenta_lemmatized.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f673f9c9-a94f-46eb-817c-6cec0f129c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta_lemmatized = lenta_lemmatized[\n",
    "    lenta_lemmatized['text_lemma'].apply(lambda x: isinstance(x, str)) & \n",
    "    lenta_lemmatized['title_lemma'].apply(lambda x: isinstance(x, str))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1333b765-4cc2-4b17-98c6-7eb70e04a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert lenta_lemmatized['text_lemma'].apply(lambda x: isinstance(x, str)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9abf7582-287f-4bea-aa9a-e84b45f085b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert lenta_lemmatized['title_lemma'].apply(lambda x: isinstance(x, str)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ca061a1-a967-4551-8468-b254705fa7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>title_lemma</th>\n",
       "      <th>text_lemma</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Телеканалы станут вещать по единому тарифу</th>\n",
       "      <td>С 1 января 2000 года все телеканалы будут опла...</td>\n",
       "      <td>Экономика</td>\n",
       "      <td>телеканал становиться вещать единый тариф</td>\n",
       "      <td>январь год весь телеканал оплачивать услуга пе...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volkswagen выкупает остатки акций \"Шкоды\"</th>\n",
       "      <td>Германский автопромышленный концерн Volkswagen...</td>\n",
       "      <td>Экономика</td>\n",
       "      <td>volkswagen выкупать остаток акция шкода</td>\n",
       "      <td>германский автопромышленный концерн volkswagen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Прибыль Тюменнефтегаза возросла в 10 раз</th>\n",
       "      <td>Нераспределенная прибыль ОАО \"Тюменнефтегаз\", ...</td>\n",
       "      <td>Экономика</td>\n",
       "      <td>прибыль тюменнефтегаз возрастать</td>\n",
       "      <td>нераспределенный прибыль оао тюменнефтегаз доч...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Крупнейшее в истории слияние компаний происходит в США</th>\n",
       "      <td>Две крупнейших телекоммуникационных компании С...</td>\n",
       "      <td>Экономика</td>\n",
       "      <td>крупный история слияние компания происходить сша</td>\n",
       "      <td>крупный телекоммуникационный компания сша дост...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ГАЗ получил четверть обещанного кредита</th>\n",
       "      <td>ОАО \"ГАЗ\" и Нижегородский банк Сбербанка Росси...</td>\n",
       "      <td>Экономика</td>\n",
       "      <td>газ получать четверть обещать кредит</td>\n",
       "      <td>оао газ нижегородский банк сбербанк россия под...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 text  \\\n",
       "title                                                                                                   \n",
       "Телеканалы станут вещать по единому тарифу          С 1 января 2000 года все телеканалы будут опла...   \n",
       "Volkswagen выкупает остатки акций \"Шкоды\"           Германский автопромышленный концерн Volkswagen...   \n",
       "Прибыль Тюменнефтегаза возросла в 10 раз            Нераспределенная прибыль ОАО \"Тюменнефтегаз\", ...   \n",
       "Крупнейшее в истории слияние компаний происходи...  Две крупнейших телекоммуникационных компании С...   \n",
       "ГАЗ получил четверть обещанного кредита             ОАО \"ГАЗ\" и Нижегородский банк Сбербанка Росси...   \n",
       "\n",
       "                                                        topic  \\\n",
       "title                                                           \n",
       "Телеканалы станут вещать по единому тарифу          Экономика   \n",
       "Volkswagen выкупает остатки акций \"Шкоды\"           Экономика   \n",
       "Прибыль Тюменнефтегаза возросла в 10 раз            Экономика   \n",
       "Крупнейшее в истории слияние компаний происходи...  Экономика   \n",
       "ГАЗ получил четверть обещанного кредита             Экономика   \n",
       "\n",
       "                                                                                         title_lemma  \\\n",
       "title                                                                                                  \n",
       "Телеканалы станут вещать по единому тарифу                 телеканал становиться вещать единый тариф   \n",
       "Volkswagen выкупает остатки акций \"Шкоды\"                    volkswagen выкупать остаток акция шкода   \n",
       "Прибыль Тюменнефтегаза возросла в 10 раз                            прибыль тюменнефтегаз возрастать   \n",
       "Крупнейшее в истории слияние компаний происходи...  крупный история слияние компания происходить сша   \n",
       "ГАЗ получил четверть обещанного кредита                         газ получать четверть обещать кредит   \n",
       "\n",
       "                                                                                           text_lemma  \n",
       "title                                                                                                  \n",
       "Телеканалы станут вещать по единому тарифу          январь год весь телеканал оплачивать услуга пе...  \n",
       "Volkswagen выкупает остатки акций \"Шкоды\"           германский автопромышленный концерн volkswagen...  \n",
       "Прибыль Тюменнефтегаза возросла в 10 раз            нераспределенный прибыль оао тюменнефтегаз доч...  \n",
       "Крупнейшее в истории слияние компаний происходи...  крупный телекоммуникационный компания сша дост...  \n",
       "ГАЗ получил четверть обещанного кредита             оао газ нижегородский банк сбербанк россия под...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenta_lemmatized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e9da5-248e-417c-b12b-a55d3bec5237",
   "metadata": {},
   "source": [
    "### Train-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bff4878-9ad4-49b7-97b0-0e0b61f02a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lenta, test_lenta = train_test_split(\n",
    "    lenta_lemmatized,\n",
    "    train_size=0.7,\n",
    "    stratify=lenta_lemmatized['topic'],\n",
    "    shuffle=True,\n",
    "    random_state=667\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38642903-cd36-4bae-b6c8-5ca8e0d390f8",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f06872d-15d2-4e26-b705-42f5811383d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фасттекст хавает данные только в таком странном виде\n",
    "def write2txt(text_list, label_list, filename):\n",
    "    save_path = os.path.join(data_dir, filename + '.txt')\n",
    "    with open(save_path, 'w') as file:\n",
    "        for sentence, label in zip(text_list, label_list):\n",
    "            file.write(f\"__label__{label} {sentence}\")\n",
    "            file.write('\\n')\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7dee896-5bdf-4eb8-b6ce-7b60d4e10c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    true_labels,\n",
    "    predicted_labels\n",
    "):\n",
    "    score_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    micro_f1 = f1_score(true_labels, predicted_labels, average='micro')\n",
    "    macro_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "    weighted_f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    \n",
    "    metric_entry = {\n",
    "        'accuracy' : score_accuracy,\n",
    "        'micro_f1' : micro_f1,\n",
    "        'macro_f1' : macro_f1,\n",
    "        'weighted_f1' : weighted_f1\n",
    "    }\n",
    "    return metric_entry\n",
    "\n",
    "def train_and_evaluate(\n",
    "    train_sentences, \n",
    "    train_labels,\n",
    "    test_sentences,\n",
    "    test_labels,\n",
    "    filename\n",
    "):\n",
    "    assert len(train_sentences) == len(train_labels)\n",
    "    assert len(test_sentences) == len(test_labels)\n",
    "    \n",
    "    save_path = write2txt(\n",
    "        train_sentences,\n",
    "        train_labels,\n",
    "        filename\n",
    "    )\n",
    "    \n",
    "    model = fasttext.train_supervised(save_path, wordNgrams=2)\n",
    "\n",
    "    delete_label_prefix = lambda word: word[len('__label__'):]\n",
    "    \n",
    "    predicted_labels_with_prefix = model.predict(test_sentences)[0]\n",
    "    predicted_labels = [delete_label_prefix(label[0]) for label in predicted_labels_with_prefix]\n",
    "\n",
    "    metric_entry = compute_metrics(test_labels, predicted_labels)\n",
    "    \n",
    "    for name, value in metric_entry.items():\n",
    "        print(f\"{name} : {round(value, 3)}\")\n",
    "\n",
    "    return metric_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6312db68-7647-412b-a4ca-c16ab6682ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 1M words\n",
      "Number of words:  43035\n",
      "Number of labels: 5\n",
      "Progress: 100.0% words/sec/thread:  613506 lr:  0.000000 avg.loss:  0.089309 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.753\n",
      "micro_f1 : 0.753\n",
      "macro_f1 : 0.498\n",
      "weighted_f1 : 0.669\n"
     ]
    }
   ],
   "source": [
    "# только на titles\n",
    "title_metrics = train_and_evaluate(\n",
    "    train_lenta['title_lemma'].tolist(),\n",
    "    train_lenta['topic'].tolist(),\n",
    "    test_lenta['title_lemma'].tolist(),\n",
    "    test_lenta['topic'].tolist(),\n",
    "    'title_lenta'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "212ced48-694b-49a1-9f91-fb2fad851869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 23M words\n",
      "Number of words:  250947\n",
      "Number of labels: 5\n",
      "Progress: 100.0% words/sec/thread: 1097572 lr:  0.000000 avg.loss:  0.109028 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.771\n",
      "micro_f1 : 0.771\n",
      "macro_f1 : 0.572\n",
      "weighted_f1 : 0.737\n"
     ]
    }
   ],
   "source": [
    "# только на full text\n",
    "text_metrics = train_and_evaluate(\n",
    "    train_lenta['text_lemma'].tolist(),\n",
    "    train_lenta['topic'].tolist(),\n",
    "    test_lenta['text_lemma'].tolist(),\n",
    "    test_lenta['topic'].tolist(),\n",
    "    'text_lemma'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d07ad24-aa76-4b78-8deb-fd82225aab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_series(lhs, rhs):\n",
    "    return lhs.str.cat(rhs, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac5562e9-256d-4558-aca1-5e94d262742e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 25M words\n",
      "Number of words:  252060\n",
      "Number of labels: 5\n",
      "Progress: 100.0% words/sec/thread: 1098749 lr:  0.000000 avg.loss:  0.110103 ETA:   0h 0m 0s 16.6% words/sec/thread: 1114218 lr:  0.083410 avg.loss:  0.356516 ETA:   0h 0m 3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.772\n",
      "micro_f1 : 0.772\n",
      "macro_f1 : 0.576\n",
      "weighted_f1 : 0.74\n"
     ]
    }
   ],
   "source": [
    "# Concatenated title and text\n",
    "title_text_metrics = train_and_evaluate(\n",
    "    join_series(train_lenta['title_lemma'], train_lenta['text_lemma']).tolist(),\n",
    "    train_lenta['topic'].tolist(),\n",
    "    join_series(test_lenta['title_lemma'], test_lenta['text_lemma']).tolist(),\n",
    "    test_lenta['topic'].tolist(),\n",
    "    'text_lemma'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ebf51-a65a-4542-a25f-3863cb3c857a",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22b96724-2949-4446-a5e3-33b842dc3898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_word_list(w2v_model, word_list):\n",
    "    vector_size = w2v_model.wv.vector_size\n",
    "    vectors = np.array([w2v_model.wv.get_vector(word) for word in word_list if word in w2v_model.wv])\n",
    "    if len(vectors) == 0:\n",
    "        vectors = np.array([[0 for index in range(vector_size)]])\n",
    "    mean_vector = vectors.mean(axis=0)\n",
    "    assert mean_vector.shape[0] == vector_size\n",
    "    return mean_vector\n",
    "\n",
    "def vectorize(w2v_model, sentences):\n",
    "    vectors = [vectorize_word_list(w2v_model, token_sentence) for token_sentence in sentences]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c84da83-ab46-4e86-9c0d-f5176daef9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "    train_text,\n",
    "    train_label,\n",
    "    test_text,\n",
    "    test_label,\n",
    "    model_name='word2vec'\n",
    "):\n",
    "    whitespace_tokenizer = nltk.WhitespaceTokenizer()\n",
    "    train_tokens = whitespace_tokenizer.tokenize_sents(train_text)\n",
    "    test_tokens = whitespace_tokenizer.tokenize_sents(test_text)\n",
    "\n",
    "    model_path = f'data/{model_name}.model'\n",
    "    if not os.path.exists(model_path):\n",
    "        vectorizer_model = Word2Vec(\n",
    "            train_tokens,\n",
    "            workers=4,\n",
    "            vector_size=300,\n",
    "            min_count=1,\n",
    "            window=5,\n",
    "            sg=1,\n",
    "            sample=1e-3\n",
    "        )\n",
    "        vectorizer_model.save(model_path)\n",
    "    else:\n",
    "        print(f'{model_path} already exists. Just loading it')\n",
    "    vectorizer_model = gensim.models.Word2Vec.load(model_path)\n",
    "\n",
    "    train_vectors = vectorize(vectorizer_model, train_tokens)\n",
    "    test_vectors = vectorize(vectorizer_model, test_tokens)\n",
    "    \n",
    "    logreg_model = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=667\n",
    "    )\n",
    "    logreg_model.fit(train_vectors, train_label)\n",
    "\n",
    "    test_predictions = logreg_model.predict(test_vectors)\n",
    "    metrics = compute_metrics(test_label, test_predictions)\n",
    "\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"{name} : {round(value, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de29513e-caf0-4127-9986-9b5e70dd484d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.896\n",
      "micro_f1 : 0.896\n",
      "macro_f1 : 0.742\n",
      "weighted_f1 : 0.886\n"
     ]
    }
   ],
   "source": [
    "# Only on titles\n",
    "pipeline(\n",
    "    train_lenta['title_lemma'].tolist(),\n",
    "    train_lenta['topic'].tolist(),\n",
    "    test_lenta['title_lemma'].tolist(),\n",
    "    test_lenta['topic'].tolist(),\n",
    "    model_name='word2vec_title'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70ea1872-738e-4bc3-83d1-4339f8b4bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.955\n",
      "micro_f1 : 0.955\n",
      "macro_f1 : 0.852\n",
      "weighted_f1 : 0.95\n"
     ]
    }
   ],
   "source": [
    "# Only on full text\n",
    "pipeline(\n",
    "    train_lenta['text_lemma'].tolist(),\n",
    "    train_lenta['topic'].tolist(),\n",
    "    test_lenta['text_lemma'].tolist(),\n",
    "    test_lenta['topic'].tolist(),\n",
    "    model_name='word2vec_text'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "646be7a2-eab1-44fb-9ba9-17f974fa111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.955\n",
      "micro_f1 : 0.955\n",
      "macro_f1 : 0.853\n",
      "weighted_f1 : 0.951\n"
     ]
    }
   ],
   "source": [
    "# Concatenated title and full text\n",
    "pipeline(\n",
    "    join_series(train_lenta['title_lemma'], train_lenta['text_lemma']).tolist(),\n",
    "    train_lenta['topic'].tolist(),\n",
    "    join_series(test_lenta['title_lemma'], test_lenta['text_lemma']).tolist(),\n",
    "    test_lenta['topic'].tolist(),\n",
    "    model_name='word2vec_title_concat_text'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3676f2e-0dbf-4a02-a6cc-3a0ef97ee801",
   "metadata": {},
   "source": [
    "Получил довольно хорошие скоры, странно что в прошлый раз получал около ~0.7\n",
    "\n",
    "Макро-ф1 просел так как классификация хуже на одном минорном классе\n",
    "\n",
    "Теперь попробую взвесить вектор весами tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e7181-9cec-4649-89b3-5d49a13cf6bd",
   "metadata": {},
   "source": [
    "### TF-IDF Weight Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "012ff7c4-e97e-48f8-8fb5-5cafdf68b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_word_list_tfidf(w2v_model, idf_values, word_list):\n",
    "    vector_size = w2v_model.wv.vector_size\n",
    "    get_idf_weight = lambda word: idf_values[word] if word in idf_values else 1\n",
    "    \n",
    "    vectors = np.array([get_idf_weight(word) * w2v_model.wv.get_vector(word) for word in word_list if word in w2v_model.wv])\n",
    "    if len(vectors) == 0:\n",
    "        vectors = np.array([[0 for index in range(vector_size)]])\n",
    "    mean_vector = vectors.mean(axis=0)\n",
    "    assert mean_vector.shape[0] == vector_size\n",
    "    return mean_vector\n",
    "\n",
    "def vectorize_tfidf(w2v_model, idf_values, sentences):\n",
    "    vectors = [vectorize_word_list_tfidf(w2v_model, idf_values, token_sentence) for token_sentence in sentences]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e1b2bbd-a1e3-4cfa-974d-724067b709c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_tfidf(\n",
    "    train_text,\n",
    "    train_label,\n",
    "    test_text,\n",
    "    test_label,\n",
    "    model_name='word2vec_tfidf'\n",
    "):\n",
    "    whitespace_tokenizer = nltk.WhitespaceTokenizer()\n",
    "    train_tokens = whitespace_tokenizer.tokenize_sents(train_text)\n",
    "    test_tokens = whitespace_tokenizer.tokenize_sents(test_text)\n",
    "\n",
    "    model_path = f'data/{model_name}.model'\n",
    "    if not os.path.exists(model_path):\n",
    "        vectorizer_model = Word2Vec(\n",
    "            train_tokens,\n",
    "            workers=4,\n",
    "            vector_size=300,\n",
    "            min_count=0,\n",
    "            window=5,\n",
    "            sg=1,\n",
    "            sample=1e-3\n",
    "        )\n",
    "        vectorizer_model.save(model_path)\n",
    "    else:\n",
    "        print(f'{model_path} already exists. Just loading it')\n",
    "    vectorizer_model = gensim.models.Word2Vec.load(model_path)\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        tokenizer=None,\n",
    "        preprocessor=None,\n",
    "        analyzer=\"word\"\n",
    "    )\n",
    "    tfidf_vectorizer.fit(train_text)\n",
    "    idf_values = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
    "    \n",
    "    \n",
    "    train_vectors = vectorize_tfidf(vectorizer_model, idf_values, train_tokens)\n",
    "    test_vectors = vectorize_tfidf(vectorizer_model, idf_values, test_tokens)\n",
    "    \n",
    "    logreg_model = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=667\n",
    "    )\n",
    "    logreg_model.fit(train_vectors, train_label)\n",
    "\n",
    "    test_predictions = logreg_model.predict(test_vectors)\n",
    "    metrics = compute_metrics(test_label, test_predictions)\n",
    "\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"{name} : {round(value, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9884a03c-1dbc-4909-8e8e-4e44c80d688d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.902\n",
      "micro_f1 : 0.902\n",
      "macro_f1 : 0.757\n",
      "weighted_f1 : 0.893\n"
     ]
    }
   ],
   "source": [
    "# Only on titles\n",
    "pipeline_tfidf(\n",
    "    train_lenta['title_lemma'].tolist(),\n",
    "    train_lenta['topic'].tolist(),\n",
    "    test_lenta['title_lemma'].tolist(),\n",
    "    test_lenta['topic'].tolist(),\n",
    "    model_name='word2vec_title_tfidf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72be1c97-5e18-46df-bfab-78025677484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.955\n",
      "micro_f1 : 0.955\n",
      "macro_f1 : 0.862\n",
      "weighted_f1 : 0.952\n"
     ]
    }
   ],
   "source": [
    "# Only on full text\n",
    "pipeline_tfidf(\n",
    "    train_lenta['text_lemma'].tolist(),\n",
    "    train_lenta['topic'].tolist(),\n",
    "    test_lenta['text_lemma'].tolist(),\n",
    "    test_lenta['topic'].tolist(),\n",
    "    model_name='word2vec_text_tfidf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "945e99a4-5aa3-4920-8494-e07c651939c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.956\n",
      "micro_f1 : 0.956\n",
      "macro_f1 : 0.861\n",
      "weighted_f1 : 0.953\n"
     ]
    }
   ],
   "source": [
    "# Concatenated title and full text\n",
    "pipeline_tfidf(\n",
    "    join_series(train_lenta['title_lemma'], train_lenta['text_lemma']).tolist(),\n",
    "    train_lenta['topic'].tolist(),\n",
    "    join_series(test_lenta['title_lemma'], test_lenta['text_lemma']).tolist(),\n",
    "    test_lenta['topic'].tolist(),\n",
    "    model_name='word2vec_title_concat_text_tfidf'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef4114-4533-4d53-9bf6-88ad82402c9a",
   "metadata": {},
   "source": [
    "Tf-Idf чуть улучшил скор. На 1 процент. Мб стоит поиграться с умножением не на 1.\n",
    "\n",
    "Размер ембеддинга 300, при тесте больших моделей на бенчмарках 300 дает очень хорошие результаты и размеры 512 и 1024 не дают сильного прироста."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
