{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649df9d8-936c-4148-9b2f-2466b14aa686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import os\n",
    "os.environ['http_proxy'] = \"http://proxy-ws.cbank.kz:8080\"\n",
    "os.environ['https_proxy'] = \"http://proxy-ws.cbank.kz:8080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f125e6-a007-44f4-bc15-02295d84886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anek.txt', 'r') as file:\n",
    "    aneki = file.read().strip().replace('<|startoftext|>', '').split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8598bc06-309d-4c7f-adde-5d588e8c9ec5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Невероятно, но предложение \"Не верьте всему, что находите в Интернете\" читается в обе стороны одинаково!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aneki[555]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b932403-a5df-46d0-adad-4613796a34fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667ab7d8-4bca-4b5c-909d-d479a6b52427",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts = train_test_split(\n",
    "    aneki,\n",
    "    test_size=0.1,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c641053-64bb-4364-a680-cb7360ef3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch_encoded = [torch.LongTensor(tokenizer.encode(text)) for text in batch]\n",
    "    batch_padded = pad_sequence(batch_encoded, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return batch_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d87e4963-8ab5-466f-b877-0c64499db994",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_texts, \n",
    "    batch_size=128, \n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_texts,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "096a658c-95f7-4272-98de-16d421ec0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84467c60-e89a-43c7-8b27-2593f254c50c",
   "metadata": {},
   "source": [
    "Формула LSTM, йуху\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "f_t=\\sigma\\left(W_f \\cdot\\left[h_{t-1}, x_t\\right]+b_f\\right) \\\\\n",
    "i_t=\\sigma\\left(W_i \\cdot\\left[h_{t-1}, x_t\\right]+b_i\\right) \\\\\n",
    "o_t=\\sigma\\left(W_o \\cdot\\left[h_{t-1}, x_t\\right]+b_o\\right) \\\\\n",
    "\\tilde{C}_t=\\tanh \\left(W_c \\cdot\\left[h_{t-1}, x_t\\right]+b_c\\right) \\\\\n",
    "C_t=f_t \\odot C_{t-1}+i_t \\odot \\tilde{C}_t \\\\\n",
    "h_t=o_t \\odot \\tanh \\left(C_t\\right)\n",
    "\\end{gathered}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba4987c3-dcf4-4539-b244-748eebdfe482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_check_shapes(lhs_shape, rhs_shape):\n",
    "    assertion_message = f\"Not equal shapes: {lhs_shape} instead of {rhs_shape}\"\n",
    "    assert lhs_shape == rhs_shape, assertion_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6174338-2ddd-4e42-8e94-c4a6a7281dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, num_tokens, embedding_size, hidden_embedding_size):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_embedding_size = hidden_embedding_size\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=num_tokens, embedding_dim=embedding_size)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self.forget_linear = nn.Linear(embedding_size + hidden_embedding_size, hidden_embedding_size)\n",
    "        self.input_linear = nn.Linear(embedding_size + hidden_embedding_size, hidden_embedding_size)\n",
    "        self.output_linear = nn.Linear(embedding_size + hidden_embedding_size, hidden_embedding_size)\n",
    "        self.short_memory_linear = nn.Linear(embedding_size + hidden_embedding_size, hidden_embedding_size)\n",
    "\n",
    "        self.classifer_to_raw_logits = nn.Linear(hidden_embedding_size, num_tokens)\n",
    "    \n",
    "    def forward(self, x_input, last_hidden_state, last_short_memory):\n",
    "        x_embed = self.embedding_layer(x_input)\n",
    "        print(\"x_embed shape: \", x_embed.shape)\n",
    "        \n",
    "        concat_state = torch.cat([x_embed, last_hidden_state], dim=-1)\n",
    "        print(\"concat state shape: \", concat_state.shape)\n",
    "\n",
    "        forget_mask = self.sigmoid(self.forget_linear(concat_state))\n",
    "        input_mask = self.sigmoid(self.input_linear(concat_state))\n",
    "        output_mask = self.sigmoid(self.output_linear(concat_state))\n",
    "        short_memory_mask = self.tanh(self.short_memory_linear(concat_state))\n",
    "        print(\"forget_mask shape: \", forget_mask.shape)\n",
    "        print(\"short memory mask: \", short_memory_mask.shape)\n",
    "\n",
    "        new_short_memory = torch.mul(forget_mask, last_short_memory) + torch.mul(input_mask, short_memory_mask)\n",
    "        print(\"new_short_memory shape: \", new_short_memory.shape)\n",
    "\n",
    "        new_hidden_state = torch.mul(output_mask, self.tanh(new_short_memory))\n",
    "        print(\"new_hidden_state shape: \", new_hidden_state.shape)\n",
    "    \n",
    "        raw_logits = self.classifer_to_raw_logits(new_hidden_state)\n",
    "        \n",
    "        return {\n",
    "            'short_memory' : new_short_memory,\n",
    "            'hidden_state' : new_hidden_state,\n",
    "            'raw_logits' : raw_logits\n",
    "        }\n",
    "\n",
    "    def get_empty_start_state(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_embedding_size, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c59eac-7641-40fd-b30f-5891eb6a0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_loop(lstm_cell: LSTMCell, batch: torch.Tensor):\n",
    "    batch_size = batch.shape[0]\n",
    "    \n",
    "    last_hidden_state = lstm_cell.get_empty_start_state(batch_size).to(device)\n",
    "    last_short_memory = lstm_cell.get_empty_start_state(batch_size).to(device)\n",
    "    \n",
    "    trans_batch = torch.transpose(batch, 1, 0)\n",
    "\n",
    "    print(\"trans_batch shape: \", trans_batch.shape)\n",
    "\n",
    "    for batch_slice in trans_batch:\n",
    "        # print(\"batch_slice shape: \", batch_slice.shape)\n",
    "        # print(\"batch_slice: \", batch_slice)\n",
    "        \n",
    "        output = lstm_cell(\n",
    "            batch_slice.to(device), \n",
    "            last_hidden_state, \n",
    "            last_short_memory\n",
    "        )\n",
    "        print(output.keys())\n",
    "\n",
    "        last_short_memory = output['short_memory']\n",
    "        last_hidden_state = output['hidden_state']\n",
    "\n",
    "        \n",
    "        \n",
    "        break\n",
    "    \n",
    "    return \"gimme gimme\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "689dd4b2-9739-4ee9-bee2-49d1c94ea9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell = LSTMCell(\n",
    "    num_tokens=len(tokenizer),\n",
    "    embedding_size=256,\n",
    "    hidden_embedding_size=256\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d14016a-fb70-4085-9e7a-be74b0fb7912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans_batch shape:  torch.Size([84, 128])\n",
      "x_embed shape:  torch.Size([128, 256])\n",
      "concat state shape:  torch.Size([128, 512])\n",
      "forget_mask shape:  torch.Size([128, 256])\n",
      "short memory mask:  torch.Size([128, 256])\n",
      "new_short_memory shape:  torch.Size([128, 256])\n",
      "new_hidden_state shape:  torch.Size([128, 256])\n",
      "dict_keys(['short_memory', 'hidden_state'])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    res = lstm_loop(lstm_cell, batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa70b3-4885-4238-adb8-f28b9dbaa30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af2961-b3fa-4232-b63b-52262acde944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
